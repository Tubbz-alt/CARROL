{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stanford SRCGAN Verification",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agupta231/CARROL/blob/develop/src/GPU-stress-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "O5dcG3YvtXfm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import gzip\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy\n",
        "from six.moves import urllib\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zds-cReitiqm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
        "WORK_DIRECTORY = 'data'\n",
        "IMAGE_SIZE = 28\n",
        "NUM_CHANNELS = 1\n",
        "PIXEL_DEPTH = 255\n",
        "NUM_LABELS = 10\n",
        "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
        "SEED = 66478  # Set to None for random seed.\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "EVAL_BATCH_SIZE = 64\n",
        "EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n",
        "\n",
        "FLAGS = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dOHXHit5tnul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data_type():\n",
        "  \"\"\"Return the type of the activations, weights, and placeholder variables.\"\"\"\n",
        "  if FLAGS.use_fp16:\n",
        "    return tf.float16\n",
        "  else:\n",
        "    return tf.float32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d-nSI5gTtpbL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def maybe_download(filename):\n",
        "  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
        "  if not tf.gfile.Exists(WORK_DIRECTORY):\n",
        "    tf.gfile.MakeDirs(WORK_DIRECTORY)\n",
        "  filepath = os.path.join(WORK_DIRECTORY, filename)\n",
        "  if not tf.gfile.Exists(filepath):\n",
        "    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
        "    with tf.gfile.GFile(filepath) as f:\n",
        "      size = f.size()\n",
        "    print('Successfully downloaded', filename, size, 'bytes.')\n",
        "  return filepath"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ds1xNMVotrL2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extract_data(filename, num_images):\n",
        "  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
        "\n",
        "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
        "  \"\"\"\n",
        "  print('Extracting', filename)\n",
        "  with gzip.open(filename) as bytestream:\n",
        "    bytestream.read(16)\n",
        "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
        "    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
        "    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
        "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6obUVoLMtvAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extract_labels(filename, num_images):\n",
        "  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
        "  print('Extracting', filename)\n",
        "  with gzip.open(filename) as bytestream:\n",
        "    bytestream.read(8)\n",
        "    buf = bytestream.read(1 * num_images)\n",
        "    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5YMOasSvtxMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fake_data(num_images):\n",
        "  \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\n",
        "  data = numpy.ndarray(\n",
        "      shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n",
        "      dtype=numpy.float32)\n",
        "  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\n",
        "  for image in xrange(num_images):\n",
        "    label = image % 2\n",
        "    data[image, :, :, 0] = label - 0.5\n",
        "    labels[image] = label\n",
        "  return data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZmGSjGltzAu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def error_rate(predictions, labels):\n",
        "  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
        "  return 100.0 - (\n",
        "      100.0 *\n",
        "      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n",
        "      predictions.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bu5fUkkvtzvL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(_):\n",
        "  if FLAGS.self_test:\n",
        "    print('Running self-test.')\n",
        "    train_data, train_labels = fake_data(256)\n",
        "    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n",
        "    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n",
        "    num_epochs = 1\n",
        "  else:\n",
        "    # Get the data.\n",
        "    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
        "    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
        "    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
        "    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    # Extract it into numpy arrays.\n",
        "    train_data = extract_data(train_data_filename, 60000)\n",
        "    train_labels = extract_labels(train_labels_filename, 60000)\n",
        "    test_data = extract_data(test_data_filename, 10000)\n",
        "    test_labels = extract_labels(test_labels_filename, 10000)\n",
        "\n",
        "    # Generate a validation set.\n",
        "    validation_data = train_data[:VALIDATION_SIZE, ...]\n",
        "    validation_labels = train_labels[:VALIDATION_SIZE]\n",
        "    train_data = train_data[VALIDATION_SIZE:, ...]\n",
        "    train_labels = train_labels[VALIDATION_SIZE:]\n",
        "    num_epochs = NUM_EPOCHS\n",
        "  train_size = train_labels.shape[0]\n",
        "\n",
        "  # This is where training samples and labels are fed to the graph.\n",
        "  # These placeholder nodes will be fed a batch of training data at each\n",
        "  # training step using the {feed_dict} argument to the Run() call below.\n",
        "  train_data_node = tf.placeholder(\n",
        "      data_type(),\n",
        "      shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
        "  train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n",
        "  eval_data = tf.placeholder(\n",
        "      data_type(),\n",
        "      shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
        "\n",
        "  # The variables below hold all the trainable weights. They are passed an\n",
        "  # initial value which will be assigned when we call:\n",
        "  # {tf.global_variables_initializer().run()}\n",
        "  conv1_weights = tf.Variable(\n",
        "      tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
        "                          stddev=0.1,\n",
        "                          seed=SEED, dtype=data_type()))\n",
        "  conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n",
        "  conv2_weights = tf.Variable(tf.truncated_normal(\n",
        "      [5, 5, 32, 64], stddev=0.1,\n",
        "      seed=SEED, dtype=data_type()))\n",
        "  conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n",
        "  fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
        "      tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
        "                          stddev=0.1,\n",
        "                          seed=SEED,\n",
        "                          dtype=data_type()))\n",
        "  fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n",
        "  fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n",
        "                                                stddev=0.1,\n",
        "                                                seed=SEED,\n",
        "                                                dtype=data_type()))\n",
        "  fc2_biases = tf.Variable(tf.constant(\n",
        "      0.1, shape=[NUM_LABELS], dtype=data_type()))\n",
        "\n",
        "  # We will replicate the model structure for the training subgraph, as well\n",
        "  # as the evaluation subgraphs, while sharing the trainable parameters.\n",
        "  def model(data, train=False):\n",
        "    \"\"\"The Model definition.\"\"\"\n",
        "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
        "    # the same size as the input). Note that {strides} is a 4D array whose\n",
        "    # shape matches the data layout: [image index, y, x, depth].\n",
        "    conv = tf.nn.conv2d(data,\n",
        "                        conv1_weights,\n",
        "                        strides=[1, 1, 1, 1],\n",
        "                        padding='SAME')\n",
        "    # Bias and rectified linear non-linearity.\n",
        "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
        "    # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
        "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
        "    pool = tf.nn.max_pool(relu,\n",
        "                          ksize=[1, 2, 2, 1],\n",
        "                          strides=[1, 2, 2, 1],\n",
        "                          padding='SAME')\n",
        "    conv = tf.nn.conv2d(pool,\n",
        "                        conv2_weights,\n",
        "                        strides=[1, 1, 1, 1],\n",
        "                        padding='SAME')\n",
        "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
        "    pool = tf.nn.max_pool(relu,\n",
        "                          ksize=[1, 2, 2, 1],\n",
        "                          strides=[1, 2, 2, 1],\n",
        "                          padding='SAME')\n",
        "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
        "    # fully connected layers.\n",
        "    pool_shape = pool.get_shape().as_list()\n",
        "    reshape = tf.reshape(\n",
        "        pool,\n",
        "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
        "    # Fully connected layer. Note that the '+' operation automatically\n",
        "    # broadcasts the biases.\n",
        "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
        "    # Add a 50% dropout during training only. Dropout also scales\n",
        "    # activations such that no rescaling is needed at evaluation time.\n",
        "    if train:\n",
        "      hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
        "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
        "\n",
        "  # Training computation: logits + cross-entropy loss.\n",
        "  logits = model(train_data_node, True)\n",
        "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      labels=train_labels_node, logits=logits))\n",
        "\n",
        "  # L2 regularization for the fully connected parameters.\n",
        "  regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
        "                  tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
        "  # Add the regularization term to the loss.\n",
        "  loss += 5e-4 * regularizers\n",
        "\n",
        "  # Optimizer: set up a variable that's incremented once per batch and\n",
        "  # controls the learning rate decay.\n",
        "  batch = tf.Variable(0, dtype=data_type())\n",
        "  # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
        "  learning_rate = tf.train.exponential_decay(\n",
        "      0.01,                # Base learning rate.\n",
        "      batch * BATCH_SIZE,  # Current index into the dataset.\n",
        "      train_size,          # Decay step.\n",
        "      0.95,                # Decay rate.\n",
        "      staircase=True)\n",
        "  # Use simple momentum for the optimization.\n",
        "  optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
        "                                         0.9).minimize(loss,\n",
        "                                                       global_step=batch)\n",
        "\n",
        "  # Predictions for the current training minibatch.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "\n",
        "  # Predictions for the test and validation, which we'll compute less often.\n",
        "  eval_prediction = tf.nn.softmax(model(eval_data))\n",
        "\n",
        "  # Small utility function to evaluate a dataset by feeding batches of data to\n",
        "  # {eval_data} and pulling the results from {eval_predictions}.\n",
        "  # Saves memory and enables this to run on smaller GPUs.\n",
        "  def eval_in_batches(data, sess):\n",
        "    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n",
        "    size = data.shape[0]\n",
        "    if size < EVAL_BATCH_SIZE:\n",
        "      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
        "    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n",
        "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
        "      end = begin + EVAL_BATCH_SIZE\n",
        "      if end <= size:\n",
        "        predictions[begin:end, :] = sess.run(\n",
        "            eval_prediction,\n",
        "            feed_dict={eval_data: data[begin:end, ...]})\n",
        "      else:\n",
        "        batch_predictions = sess.run(\n",
        "            eval_prediction,\n",
        "            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
        "        predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
        "    return predictions\n",
        "\n",
        "  # Create a local session to run the training.\n",
        "  start_time = time.time()\n",
        "  with tf.Session() as sess:\n",
        "    # Run all the initializers to prepare the trainable parameters.\n",
        "    tf.global_variables_initializer().run()\n",
        "    print('Initialized!')\n",
        "    # Loop through training steps.\n",
        "    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n",
        "      # Compute the offset of the current minibatch in the data.\n",
        "      # Note that we could use better randomization across epochs.\n",
        "      offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
        "      batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
        "      batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
        "      # This dictionary maps the batch data (as a numpy array) to the\n",
        "      # node in the graph it should be fed to.\n",
        "      feed_dict = {train_data_node: batch_data,\n",
        "                   train_labels_node: batch_labels}\n",
        "      # Run the optimizer to update weights.\n",
        "      sess.run(optimizer, feed_dict=feed_dict)\n",
        "      # print some extra information once reach the evaluation frequency\n",
        "      if step % EVAL_FREQUENCY == 0:\n",
        "        # fetch some extra nodes' data\n",
        "        l, lr, predictions = sess.run([loss, learning_rate, train_prediction],\n",
        "                                      feed_dict=feed_dict)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        start_time = time.time()\n",
        "        print('Step %d (epoch %.2f), %.1f ms' %\n",
        "              (step, float(step) * BATCH_SIZE / train_size,\n",
        "               1000 * elapsed_time / EVAL_FREQUENCY))\n",
        "        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
        "        print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
        "        print('Validation error: %.1f%%' % error_rate(\n",
        "            eval_in_batches(validation_data, sess), validation_labels))\n",
        "        sys.stdout.flush()\n",
        "    # Finally print the result!\n",
        "    test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n",
        "    print('Test error: %.1f%%' % test_error)\n",
        "    if FLAGS.self_test:\n",
        "      print('test_error', test_error)\n",
        "      assert test_error == 0.0, 'expected 0.0 test_error, got %.2f' % (\n",
        "          test_error,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5mBzolMt6GP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7153
        },
        "outputId": "16636f7e-66ad-4e13-95d7-ff6843345ba2"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\n",
        "      '--use_fp16',\n",
        "      default=False,\n",
        "      help='Use half floats instead of full floats if True.',\n",
        "      action='store_true')\n",
        "  parser.add_argument(\n",
        "      '--self_test',\n",
        "      default=False,\n",
        "      action='store_true',\n",
        "      help='True if running a self test.')\n",
        "\n",
        "  FLAGS, unparsed = parser.parse_known_args()\n",
        "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "Initialized!\n",
            "Step 0 (epoch 0.00), 9.9 ms\n",
            "Minibatch loss: 8.334, learning rate: 0.010000\n",
            "Minibatch error: 85.9%\n",
            "Validation error: 84.6%\n",
            "Step 100 (epoch 0.12), 5.8 ms\n",
            "Minibatch loss: 3.244, learning rate: 0.010000\n",
            "Minibatch error: 4.7%\n",
            "Validation error: 8.0%\n",
            "Step 200 (epoch 0.23), 5.5 ms\n",
            "Minibatch loss: 3.388, learning rate: 0.010000\n",
            "Minibatch error: 14.1%\n",
            "Validation error: 4.2%\n",
            "Step 300 (epoch 0.35), 5.4 ms\n",
            "Minibatch loss: 3.194, learning rate: 0.010000\n",
            "Minibatch error: 6.2%\n",
            "Validation error: 3.0%\n",
            "Step 400 (epoch 0.47), 5.4 ms\n",
            "Minibatch loss: 3.242, learning rate: 0.010000\n",
            "Minibatch error: 7.8%\n",
            "Validation error: 2.7%\n",
            "Step 500 (epoch 0.58), 5.4 ms\n",
            "Minibatch loss: 3.203, learning rate: 0.010000\n",
            "Minibatch error: 4.7%\n",
            "Validation error: 2.5%\n",
            "Step 600 (epoch 0.70), 5.4 ms\n",
            "Minibatch loss: 3.110, learning rate: 0.010000\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.9%\n",
            "Step 700 (epoch 0.81), 5.4 ms\n",
            "Minibatch loss: 2.948, learning rate: 0.010000\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 2.2%\n",
            "Step 800 (epoch 0.93), 5.4 ms\n",
            "Minibatch loss: 3.045, learning rate: 0.010000\n",
            "Minibatch error: 6.2%\n",
            "Validation error: 2.1%\n",
            "Step 900 (epoch 1.05), 5.6 ms\n",
            "Minibatch loss: 2.926, learning rate: 0.009500\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.8%\n",
            "Step 1000 (epoch 1.16), 5.7 ms\n",
            "Minibatch loss: 2.877, learning rate: 0.009500\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.8%\n",
            "Step 1100 (epoch 1.28), 5.6 ms\n",
            "Minibatch loss: 2.830, learning rate: 0.009500\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.6%\n",
            "Step 1200 (epoch 1.40), 5.6 ms\n",
            "Minibatch loss: 2.939, learning rate: 0.009500\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.5%\n",
            "Step 1300 (epoch 1.51), 5.4 ms\n",
            "Minibatch loss: 2.797, learning rate: 0.009500\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.9%\n",
            "Step 1400 (epoch 1.63), 5.3 ms\n",
            "Minibatch loss: 2.840, learning rate: 0.009500\n",
            "Minibatch error: 4.7%\n",
            "Validation error: 1.5%\n",
            "Step 1500 (epoch 1.75), 5.3 ms\n",
            "Minibatch loss: 2.895, learning rate: 0.009500\n",
            "Minibatch error: 4.7%\n",
            "Validation error: 1.2%\n",
            "Step 1600 (epoch 1.86), 5.4 ms\n",
            "Minibatch loss: 2.712, learning rate: 0.009500\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.5%\n",
            "Step 1700 (epoch 1.98), 5.5 ms\n",
            "Minibatch loss: 2.655, learning rate: 0.009500\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.5%\n",
            "Step 1800 (epoch 2.09), 5.4 ms\n",
            "Minibatch loss: 2.667, learning rate: 0.009025\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.2%\n",
            "Step 1900 (epoch 2.21), 5.3 ms\n",
            "Minibatch loss: 2.618, learning rate: 0.009025\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.1%\n",
            "Step 2000 (epoch 2.33), 5.4 ms\n",
            "Minibatch loss: 2.591, learning rate: 0.009025\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.3%\n",
            "Step 2100 (epoch 2.44), 5.4 ms\n",
            "Minibatch loss: 2.579, learning rate: 0.009025\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.2%\n",
            "Step 2200 (epoch 2.56), 5.4 ms\n",
            "Minibatch loss: 2.599, learning rate: 0.009025\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.2%\n",
            "Step 2300 (epoch 2.68), 5.3 ms\n",
            "Minibatch loss: 2.607, learning rate: 0.009025\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.2%\n",
            "Step 2400 (epoch 2.79), 5.4 ms\n",
            "Minibatch loss: 2.497, learning rate: 0.009025\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.1%\n",
            "Step 2500 (epoch 2.91), 5.4 ms\n",
            "Minibatch loss: 2.473, learning rate: 0.009025\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.2%\n",
            "Step 2600 (epoch 3.03), 5.4 ms\n",
            "Minibatch loss: 2.451, learning rate: 0.008574\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.3%\n",
            "Step 2700 (epoch 3.14), 5.3 ms\n",
            "Minibatch loss: 2.465, learning rate: 0.008574\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.2%\n",
            "Step 2800 (epoch 3.26), 5.4 ms\n",
            "Minibatch loss: 2.441, learning rate: 0.008574\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.2%\n",
            "Step 2900 (epoch 3.37), 5.4 ms\n",
            "Minibatch loss: 2.471, learning rate: 0.008574\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.1%\n",
            "Step 3000 (epoch 3.49), 5.4 ms\n",
            "Minibatch loss: 2.392, learning rate: 0.008574\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.1%\n",
            "Step 3100 (epoch 3.61), 5.3 ms\n",
            "Minibatch loss: 2.363, learning rate: 0.008574\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.0%\n",
            "Step 3200 (epoch 3.72), 5.4 ms\n",
            "Minibatch loss: 2.352, learning rate: 0.008574\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.2%\n",
            "Step 3300 (epoch 3.84), 5.5 ms\n",
            "Minibatch loss: 2.330, learning rate: 0.008574\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.2%\n",
            "Step 3400 (epoch 3.96), 5.5 ms\n",
            "Minibatch loss: 2.303, learning rate: 0.008574\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.0%\n",
            "Step 3500 (epoch 4.07), 5.3 ms\n",
            "Minibatch loss: 2.324, learning rate: 0.008145\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.0%\n",
            "Step 3600 (epoch 4.19), 5.4 ms\n",
            "Minibatch loss: 2.252, learning rate: 0.008145\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 3700 (epoch 4.31), 5.4 ms\n",
            "Minibatch loss: 2.229, learning rate: 0.008145\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.1%\n",
            "Step 3800 (epoch 4.42), 5.4 ms\n",
            "Minibatch loss: 2.217, learning rate: 0.008145\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 3900 (epoch 4.54), 5.4 ms\n",
            "Minibatch loss: 2.280, learning rate: 0.008145\n",
            "Minibatch error: 4.7%\n",
            "Validation error: 1.0%\n",
            "Step 4000 (epoch 4.65), 5.4 ms\n",
            "Minibatch loss: 2.283, learning rate: 0.008145\n",
            "Minibatch error: 6.2%\n",
            "Validation error: 1.1%\n",
            "Step 4100 (epoch 4.77), 5.4 ms\n",
            "Minibatch loss: 2.203, learning rate: 0.008145\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.0%\n",
            "Step 4200 (epoch 4.89), 5.3 ms\n",
            "Minibatch loss: 2.155, learning rate: 0.008145\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.1%\n",
            "Step 4300 (epoch 5.00), 5.4 ms\n",
            "Minibatch loss: 2.217, learning rate: 0.007738\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.0%\n",
            "Step 4400 (epoch 5.12), 5.4 ms\n",
            "Minibatch loss: 2.145, learning rate: 0.007738\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.1%\n",
            "Step 4500 (epoch 5.24), 5.4 ms\n",
            "Minibatch loss: 2.179, learning rate: 0.007738\n",
            "Minibatch error: 6.2%\n",
            "Validation error: 1.0%\n",
            "Step 4600 (epoch 5.35), 5.4 ms\n",
            "Minibatch loss: 2.082, learning rate: 0.007738\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 4700 (epoch 5.47), 5.4 ms\n",
            "Minibatch loss: 2.087, learning rate: 0.007738\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.9%\n",
            "Step 4800 (epoch 5.59), 5.3 ms\n",
            "Minibatch loss: 2.088, learning rate: 0.007738\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.0%\n",
            "Step 4900 (epoch 5.70), 5.4 ms\n",
            "Minibatch loss: 2.080, learning rate: 0.007738\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.0%\n",
            "Step 5000 (epoch 5.82), 5.5 ms\n",
            "Minibatch loss: 2.098, learning rate: 0.007738\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 0.9%\n",
            "Step 5100 (epoch 5.93), 5.5 ms\n",
            "Minibatch loss: 2.002, learning rate: 0.007738\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 5200 (epoch 6.05), 5.3 ms\n",
            "Minibatch loss: 2.100, learning rate: 0.007351\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.0%\n",
            "Step 5300 (epoch 6.17), 5.4 ms\n",
            "Minibatch loss: 1.970, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 5400 (epoch 6.28), 5.5 ms\n",
            "Minibatch loss: 1.957, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 5500 (epoch 6.40), 5.5 ms\n",
            "Minibatch loss: 1.977, learning rate: 0.007351\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 1.0%\n",
            "Step 5600 (epoch 6.52), 5.5 ms\n",
            "Minibatch loss: 1.940, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 5700 (epoch 6.63), 5.5 ms\n",
            "Minibatch loss: 1.911, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 5800 (epoch 6.75), 5.5 ms\n",
            "Minibatch loss: 1.901, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 5900 (epoch 6.87), 5.4 ms\n",
            "Minibatch loss: 1.889, learning rate: 0.007351\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6000 (epoch 6.98), 5.5 ms\n",
            "Minibatch loss: 1.932, learning rate: 0.007351\n",
            "Minibatch error: 3.1%\n",
            "Validation error: 0.9%\n",
            "Step 6100 (epoch 7.10), 5.3 ms\n",
            "Minibatch loss: 1.868, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6200 (epoch 7.21), 5.4 ms\n",
            "Minibatch loss: 1.848, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6300 (epoch 7.33), 5.5 ms\n",
            "Minibatch loss: 1.834, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6400 (epoch 7.45), 5.4 ms\n",
            "Minibatch loss: 1.858, learning rate: 0.006983\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.8%\n",
            "Step 6500 (epoch 7.56), 5.4 ms\n",
            "Minibatch loss: 1.808, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 6600 (epoch 7.68), 5.4 ms\n",
            "Minibatch loss: 1.822, learning rate: 0.006983\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.0%\n",
            "Step 6700 (epoch 7.80), 5.3 ms\n",
            "Minibatch loss: 1.780, learning rate: 0.006983\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 6800 (epoch 7.91), 5.3 ms\n",
            "Minibatch loss: 1.783, learning rate: 0.006983\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 1.0%\n",
            "Step 6900 (epoch 8.03), 5.4 ms\n",
            "Minibatch loss: 1.764, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 7000 (epoch 8.15), 5.3 ms\n",
            "Minibatch loss: 1.750, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 7100 (epoch 8.26), 5.3 ms\n",
            "Minibatch loss: 1.736, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 7200 (epoch 8.38), 5.4 ms\n",
            "Minibatch loss: 1.737, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 7300 (epoch 8.49), 5.5 ms\n",
            "Minibatch loss: 1.741, learning rate: 0.006634\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.9%\n",
            "Step 7400 (epoch 8.61), 5.4 ms\n",
            "Minibatch loss: 1.708, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 7500 (epoch 8.73), 5.4 ms\n",
            "Minibatch loss: 1.706, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 7600 (epoch 8.84), 5.5 ms\n",
            "Minibatch loss: 1.798, learning rate: 0.006634\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.9%\n",
            "Step 7700 (epoch 8.96), 5.4 ms\n",
            "Minibatch loss: 1.667, learning rate: 0.006634\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 1.0%\n",
            "Step 7800 (epoch 9.08), 5.4 ms\n",
            "Minibatch loss: 1.656, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 7900 (epoch 9.19), 5.3 ms\n",
            "Minibatch loss: 1.645, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 8000 (epoch 9.31), 5.5 ms\n",
            "Minibatch loss: 1.645, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.9%\n",
            "Step 8100 (epoch 9.43), 5.4 ms\n",
            "Minibatch loss: 1.630, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 8200 (epoch 9.54), 5.3 ms\n",
            "Minibatch loss: 1.624, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 8300 (epoch 9.66), 5.3 ms\n",
            "Minibatch loss: 1.615, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.8%\n",
            "Step 8400 (epoch 9.77), 5.4 ms\n",
            "Minibatch loss: 1.595, learning rate: 0.006302\n",
            "Minibatch error: 0.0%\n",
            "Validation error: 0.7%\n",
            "Step 8500 (epoch 9.89), 5.3 ms\n",
            "Minibatch loss: 1.604, learning rate: 0.006302\n",
            "Minibatch error: 1.6%\n",
            "Validation error: 0.8%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6d432105bf3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0munparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2cb3fa289b67>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    178\u001b[0m                    train_labels_node: batch_labels}\n\u001b[1;32m    179\u001b[0m       \u001b[0;31m# Run the optimizer to update weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m       \u001b[0;31m# print some extra information once reach the evaluation frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVAL_FREQUENCY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "bmnoRSlqqrqu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}