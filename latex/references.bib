@article{Dong2016,
abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
archivePrefix = {arXiv},
arxivId = {1501.00092},
author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
doi = {10.1109/TPAMI.2015.2439281},
eprint = {1501.00092},
file = {:home/agupta/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2016 - Image Super-Resolution Using Deep Convolutional Networks.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Super-resolution,deep convolutional neural networks,sparse coding},
pmid = {26761735},
title = {{Image Super-Resolution Using Deep Convolutional Networks}},
year = {2016}
}
@techreport{Kovalenko,
author = {Kovalenko, Boris},
file = {::},
title = {{Super resolution with Generative Adversarial Networks}},
url = {http://cs231n.stanford.edu/reports/2017/pdfs/17.pdf}
}
@techreport{Ledig,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper con-volutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4Ã— upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
archivePrefix = {arXiv},
arxivId = {1609.04802v5},
author = {Ledig, Christian and Theis, Lucas and Husz{\'{a}}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and {Shi Twitter}, Wenzhe},
eprint = {1609.04802v5},
file = {::},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {https://arxiv.org/pdf/1609.04802.pdf}
}
@techreport{Dong,
abstract = {As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) [1, 2] has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We redesign the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network , then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.},
author = {Dong, Chao and Loy, Chen Change and Tang, Xiaoou},
file = {::},
title = {{Accelerating the Super-Resolution Convolutional Neural Network}},
url = {http://mmlab.ie.cuhk.edu.hk/}
}
@article{Kanaev2013,
abstract = {Multi-frame super-resolution algorithms offer resolution enhancement for sequences of images with sampling limited resolution. However, classical approaches have been constrained by the accuracy of motion estimation while nonlocal approaches that use implicit motion estimation have attained only modest resolution improvement. In this paper, we propose a new multi-frame optical flow based super-resolution algorithm, which provides significant resolution enhancement for image sequences containing complex motion. The algorithm uses the standard camera image formation model and a variational super-resolution formulation with an anisotropic smoothness term adapting to local image structures. The key elements enabling super-resolution of complex motion patterns are the computation of two-way optical flow between the images and use of two corresponding uncertainty measures that approximate the optical flow interpolation error. Using the developed algorithm, we are able to demonstrate super-resolution of images for which optical flow estimation experiences near breakdown, due to the complexity of the motion patterns and the large magnitudes of the displacements. In comparison, we show that for these images some conventional super-resolution approaches fail, while others including nonlocal super-resolution technique produce distortions and provide lower (1-1.8dB) image quality enhancement compared to the proposed algorithm.},
author = {Kanaev, A V and Miller, C W and Park, S C and Park, M K and Kang, M G},
doi = {10.1364/OE.21.019850},
file = {::},
title = {{Image processing; (100.6640) Superresolution; (150.4620) Optical flow}},
url = {https://www.osapublishing.org/DirectPDFAccess/B916EF4D-C653-CEB9-B7ED99B8AA5A5097{\_}260337/oe-21-17-19850.pdf?da=1{\&}id=260337{\&}seq=0{\&}mobile=no},
year = {2013}
}
@techreport{Chen,
abstract = {1. Abstract We investigated the problem of image super-resolution, a classic and highly-applicable task in computer vision. Recently , super-resolution has been very successful at low up-scaling factors (2-4x) with GANs. In this paper, we proposed several methods to introduce auxiliary, conditional information into a super-resolution model to produce images more tuned to the human eye. We showed that our model, trained on the MNIST and CelebA datasets and conditioned on digits/facial attributes respectively, helped constrain the solution-space of the super-resolution task to produce more accurate upscaled images.},
author = {Chen, Vincent and Puzon, Liezl and Wadsworth, Christina},
file = {:home/agupta/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Puzon, Wadsworth - Unknown - Class-Conditional Superresolution with GANs(2).pdf:pdf},
title = {{Class-Conditional Superresolution with GANs}},
url = {http://cs231n.stanford.edu/reports/2017/pdfs/314.pdf}
}
@techreport{Freeman2002,
abstract = {Polygon-based representations of 3D objects offer resolution independence over a wide range of scales. With this approach, object boundaries remain sharp when we zoom in on an object until very close range, where faceting appears due to finite polygon size (see Figure 1). However, constructing polygon models for complex, real-world objects can be difficult. Image-based rendering (IBR), a complementary approach for representing and rendering objects, uses cameras to obtain rich models directly from real-world data. Unfortunately, these representations no longer have resolution independence. When we enlarge a bitmapped image, we get a blurry result. Figure 2 shows the problem for an IBR version of a teapot image, rich with real-world detail. Standard pixel interpolation methods, such as pixel replication (Figures 2b and 2c) and cubic-spline interpolation (Fig-ures 2d and 2e), introduce artifacts or blur edges. For images enlarged three octaves (fac-tors of two) such as these, sharpening the interpolated result has little useful effect (Figures 2f and 2g). We call methods for achieving high-resolution enlargements of pixel-based images super-resolution algorithms. Many applications in graphics or image processing could benefit from such resolution independence , including IBR, texture mapping, enlarging consumer photographs, and converting NTSC video content to high-definition television. We built on another training-based super-resolution algorithm 1 and developed a faster and simpler algorithm for one-pass super-resolution. (The one-pass, example-based algorithm gives the enlargements in Figures 2h and 2i.) Our algorithm requires only a nearest-neighbor search in the training set for a vector derived from each patch of local image data. This one-pass super-resolution algorithm is a step toward achieving resolution independence in image-based representations. We don't expect perfect resolution independence-even the polygon representation doesn't have that-but increasing the resolution independence of pixel-based representations is an important task for IBR. Example-based approaches Super-resolution relates to image interpolation-how should we interpolate between the digital samples of a photograph? Researchers have long studied this problem , although only recently with machine learning or sampling approaches. (See the "Related Approaches" sidebar for more details.) Three complimentary ways exist for increasing an image's apparent resolution: 0272-1716/02/{\$}17.00 1 (a) When we model an object with traditional polygon techniques, it lacks some of the richness of real-world objects but behaves properly under enlargement. (b) The teapot's edge remains sharp when we enlarge it. (a) (b)},
author = {Freeman, William T and Jones, Thouis R and Pasztor, Egon C},
file = {::},
title = {{Example-Based Super-Resolution}},
url = {http://www.altamira.com},
year = {2002}
}
